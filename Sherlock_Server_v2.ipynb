{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l5k7alPmGTOc"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqssbxuwtQZO"
      },
      "source": [
        "The !pip install transformers installs the pre compiled transformers package (.whl file) from pypi\n",
        "\n",
        "import transformers\n",
        "transformers.__version__\n",
        "\n",
        "Prints ---> 4.27.4\n",
        "\n",
        "Whereas the !pip install git+https://github.com/huggingface/transformers.git , check-out the latest version from git repository and compiles it on the google colab environment and then installs the package.\n",
        "It installs the latest development version.\n",
        "\n",
        "import transformers\n",
        "transformers.__version__\n",
        "\n",
        "Prints ---> 4.28.0.dev0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvvLEiYsAQZ4",
        "outputId": "7f125832-247a-43cf-b979-dee99389a3c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-y4jg8t7e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-y4jg8t7e\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 4f58fc9c823c43b67a6ce1c44be28f8aecc3c8d9\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.0.dev0) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.0.dev0) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (2025.1.31)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.52.0.dev0-py3-none-any.whl size=11447297 sha256=809449496cbfd8588f417af5e1b5f04d8205c748e48f2ad25b1a06872b273276\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kti0tejt/wheels/32/4b/78/f195c684dd3a9ed21f3b39fe8f85b48df7918581b6437be143\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "Successfully installed transformers-4.52.0.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "# !pip install -q tensorflow\n",
        "#root"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tokenizer that turns input text into tokens understandable by the model.\n",
        "\n",
        "A tokenizer is the bridge between raw text (what humans read) and model input (what machines understand).\n",
        "It breaks down sentences into small pieces called tokens that the model can process.\n",
        "\n",
        "Text → Tokenizer → Tokens → Model\n",
        "\n",
        "Model Output → Tokens → Tokenizer → Human-readable Text"
      ],
      "metadata": {
        "id": "ajfyw3bkVTg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important parameters used in text generation:\n",
        "\n",
        "* do_sample=True: Enables sampling to make the output creative.\n",
        "\n",
        "  Input: \"Once upon a time,\"\n",
        "\n",
        "  Output 1: \"Once upon a time, a dragon flew over the hills.\"\n",
        "\n",
        "  Output 2: \"Once upon a time, a lonely knight wandered the desert.\"\n",
        "\n",
        "  (Different outputs each time)\n",
        "\n",
        "* max_length: Controls how long the generated text can be.\n",
        "\n",
        "  max_length=30\n",
        "\n",
        "  Output: A short paragraph.\n",
        "\n",
        "  max_length=100\n",
        "\n",
        "  Output: A much longer, story-like paragraph.\n",
        "\n",
        "* top_k: sampling limits the model to only considering the top-k highest probability tokens when picking the next word.\n",
        "\n",
        "  If top_k=50, the model picks randomly among the 50 most likely next words.\n",
        "\n",
        "  If top_k=0, no restriction: it considers the full vocabulary (full randomness).\n",
        "\n",
        "* temperature: controls the creativity of the model (i.e., randomness); lower values make output more focused.\n",
        "\n",
        "  temperature=0.2\n",
        "\n",
        "  Input: \"The cat sat\" Output: \"The cat sat on the mat and fell asleep.\"\n",
        "\n",
        "  temperature=1.3\n",
        "\n",
        "  Input: \"The cat sat\" Output: \"The cat sat weaving symphonies into golden dreams across universes.\""
      ],
      "metadata": {
        "id": "Jijk5ABcVJpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFlunArpsH9C",
        "outputId": "c34872ef-1851-4c9d-ec9c-79e57280ce8a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zi1CS5rIGm21"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Replace with the actual model path (adjust if you're not in Colab)\n",
        "model_path = \"/content/drive/MyDrive/HolmesModel\"\n",
        "\n",
        "# Load fine-tuned tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "\n",
        "import torch\n",
        "\n",
        "# Set device properly for PyTorch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "def generate_response(input_text, max_length):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        do_sample=True,\n",
        "        max_length=max_length,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    response_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return response_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6jKypiyrwqr"
      },
      "source": [
        "### Install Flask-ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SzAQ5uQYetf",
        "outputId": "955f5397-56c8-4e38-c80d-623d97f93080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.4-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.4-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.4\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Replace with your ngrok auth token\n",
        "!ngrok config add-authtoken 2vpKSeY2c2BbKT07CrCnqf3YOhm_5BrVgTTEvvbmBRSo18pGi\n",
        "#ngrok.set_auth_token(\"2vpKSeY2c2BbKT07CrCnqf3YOhm_5BrVgTTEvvbmBRSo18pGi\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJX9OP4xXZBN",
        "outputId": "3c427f70-a75e-43e6-e134-eeecd15e8e1a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AorvifJBr0VA"
      },
      "source": [
        "### Start the Flask Server\n",
        "\n",
        "Copy the generated ngrok server url and use in the client code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_response(raw_response, query):\n",
        "    \"\"\"\n",
        "    Cleans the raw GPT-2 output to make it more suitable for a chatbot interface.\n",
        "\n",
        "    Args:\n",
        "        raw_response (str): The raw text generated by GPT-2\n",
        "        query (str): The original user query (for context-aware cleaning)\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned, more natural sounding response\n",
        "    \"\"\"\n",
        "    # 1. Remove the original query if it's repeated at the start\n",
        "    if raw_response.startswith(query):\n",
        "        cleaned = raw_response[len(query):].lstrip(\",.!? \\n\")\n",
        "    else:\n",
        "        cleaned = raw_response\n",
        "\n",
        "    # 2. Trim everything after the last complete sentence\n",
        "    last_punctuation = max(\n",
        "        cleaned.rfind(\".\"),\n",
        "        cleaned.rfind(\"!\"),\n",
        "        cleaned.rfind(\"?\"),\n",
        "        cleaned.rfind(\"\\n\")\n",
        "    )\n",
        "\n",
        "    if last_punctuation > 0:\n",
        "        cleaned = cleaned[:last_punctuation + 1]\n",
        "\n",
        "    # 3. Remove any repetitive patterns (common in GPT-2 outputs)\n",
        "    sentences = [s.strip() for s in cleaned.split(\". \") if s.strip()]\n",
        "    if len(sentences) > 1 and sentences[-1] in sentences[:-1]:\n",
        "        cleaned = \". \".join(sentences[:-1]) + \".\"\n",
        "\n",
        "    # 4. Fix common formatting issues\n",
        "    cleaned = cleaned.replace(\"  \", \" \")  # Double spaces\n",
        "    cleaned = cleaned.replace(\"\\n\", \" \")  # Newlines\n",
        "    cleaned = cleaned.replace('\"\"', '\"')  # Double quotes\n",
        "\n",
        "    # 5. Capitalize first letter and ensure ends with punctuation\n",
        "    cleaned = cleaned.strip()\n",
        "    if cleaned:\n",
        "        cleaned = cleaned[0].upper() + cleaned[1:]\n",
        "        if cleaned[-1] not in {\".\", \"!\", \"?\"}:\n",
        "            cleaned += \".\"\n",
        "\n",
        "    # 6. Specific cleaning for Sherlock persona\n",
        "    if \"Sherlock\" in query or \"Holmes\" in query:\n",
        "        sherlock_signatures = [\n",
        "            \" Elementary, my dear Watson.\",\n",
        "            \" As I often tell Watson...\",\n",
        "            \" The game is afoot!\"\n",
        "        ]\n",
        "        if not any(sig in cleaned for sig in sherlock_signatures):\n",
        "            cleaned += sherlock_signatures[0]\n",
        "\n",
        "    return cleaned"
      ],
      "metadata": {
        "id": "PMkzAWwzv9ds"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-cors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysdNFM6-s3Ur",
        "outputId": "2172c298-a45e-4edc-e8e9-785666f6d5c0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask-cors\n",
            "  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n",
            "Requirement already satisfied: flask>=0.9 in /usr/local/lib/python3.11/dist-packages (from flask-cors) (3.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.11/dist-packages (from flask-cors) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug>=0.7->flask-cors) (3.0.2)\n",
            "Downloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: flask-cors\n",
            "Successfully installed flask-cors-5.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2umwAnC1GlU7",
        "outputId": "7b194d18-fb96-4d76-8154-cdcea1a695ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * ngrok tunnel: https://ab49-34-45-116-75.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [18/Apr/2025 19:43:27] \"OPTIONS /chatbot HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/Apr/2025 19:43:32] \"POST /chatbot HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/Apr/2025 19:44:18] \"GET /health HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/Apr/2025 19:44:33] \"POST /chatbot HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/Apr/2025 19:46:13] \"POST /chatbot HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/Apr/2025 19:47:01] \"POST /chatbot HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/Apr/2025 19:47:38] \"POST /chatbot HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/Apr/2025 19:49:11] \"OPTIONS /chatbot HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/Apr/2025 19:49:14] \"POST /chatbot HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/Apr/2025 19:49:43] \"OPTIONS /chatbot HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/Apr/2025 19:49:46] \"POST /chatbot HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/Apr/2025 19:50:01] \"OPTIONS /chatbot HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [18/Apr/2025 19:50:03] \"POST /chatbot HTTP/1.1\" 200 -\n",
            "WARNING:pyngrok.process.ngrok:t=2025-04-18T19:50:38+0000 lvl=warn msg=\"Stopping forwarder\" name=http-5000-07a4ff64-dd7e-4aeb-b5af-a14c0fa75680 acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "from flask_cors import CORS\n",
        "from datetime import datetime  # Add this import\n",
        "import logging  # For better error handling\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Configure CORS with additional security options\n",
        "CORS(app, resources={\n",
        "    r\"/chatbot\": {\n",
        "        \"origins\": [\"*\"],  # In production, replace with your frontend URL\n",
        "        \"methods\": [\"POST\", \"OPTIONS\"],\n",
        "        \"allow_headers\": [\"Content-Type\"]\n",
        "    },\n",
        "    r\"/health\": {\n",
        "        \"origins\": \"*\",\n",
        "        \"methods\": [\"GET\"]\n",
        "    }\n",
        "})\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    response = {'text': 'Success', 'timestamp': datetime.now().isoformat()}\n",
        "    return jsonify(response)\n",
        "\n",
        "@app.route('/chatbot', methods=['POST'])\n",
        "def chatbot():\n",
        "    try:\n",
        "        # Validate input\n",
        "        if not request.is_json:\n",
        "            return jsonify({'error': 'Request must be JSON'}), 400\n",
        "\n",
        "        input_text = request.json.get('text')\n",
        "        if not input_text or not input_text.strip():\n",
        "            return jsonify({'error': 'Text cannot be empty'}), 400\n",
        "\n",
        "        # Generate and clean response\n",
        "        raw_response = generate_response(input_text, max_length=50)\n",
        "        cleaned_text = clean_response(raw_response, input_text)\n",
        "\n",
        "        response = {\n",
        "            'raw_response': raw_response,\n",
        "            'cleaned_response': cleaned_text,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "        return jsonify(response)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in chatbot endpoint: {str(e)}\")\n",
        "        return jsonify({'error': 'Internal server error'}), 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    port = 5000\n",
        "    try:\n",
        "        public_url = ngrok.connect(port).public_url\n",
        "        print(f\" * ngrok tunnel: {public_url}\")\n",
        "        app.run(port=port)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to start server: {str(e)}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uKwotOuGxQW_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}